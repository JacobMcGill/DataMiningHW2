---
title: "DHW2"
authors: "Daniel Oliner, Musab Alquwaee, Jacob McGill"
output: md_document
date: "2024-02-07"
---
# ECO 395M HW 1: Daniel Oliner, Musab Alquwaee, Jacob McGill

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include = FALSE}
library(pROC)
library(lubridate)
library(tidyverse)
library(mosaic)
library(ggplot2)
library(rsample)
library(rsample)
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(kknn)
library(reshape2)
library(dplyr)

credit_data = read.csv("C://Users/jacob/Downloads/german_credit.csv", header = TRUE)
hotels_dev = read.csv("C://Users/jacob/Downloads/hotels_dev.csv", header = TRUE) 
mushrooms = read.csv("C://Users/jacob/Downloads/mushrooms.csv", header = TRUE)
```

```{r, include=FALSE}
data(SaratogaHouses)
glimpse(SaratogaHouses)

saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)

lm1 = lm(price ~ lotSize + bedrooms + bathrooms, data=saratoga_train)
lm2 = lm(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
lm3 = lm(price ~ (. - pctCollege - sewer - waterfront - landValue - newConstruction)^2, data=saratoga_train)

rmse(lm1, saratoga_test)
rmse(lm2, saratoga_test)
rmse(lm3, saratoga_test)

```

# Q1: Executive Summary

Focusing on the data set on house prices in Saratoga, NY, this report presents a comparative analysis of two model classes: Linear Models and K-Nearest-Neighbors. We will begin by building a comprehensive linear model, then a KNN model, then will evaluate the two models' efficacy and predictive accuracy by comparing their out-of-sample mean-squared error. The goal is to identify a model that combines high predictive accuracy with practicality for use by the local taxing authority.

## Question 1: Part 1: Building an Enhanced Linear Model 

```{r, echo=FALSE}

lm_full <- lm(price ~ ., data = saratoga_train)

# Apply backward selection
lm_backward <- step(lm_full, direction = "backward")

# Summary of the model after backward selection
summary(lm_backward)

backward_formula <- formula(lm_backward)


```

Started by running an "everything and the kitchen sink" model and then used a backwards selection approach to iteratively delete statistically insignificant variables in order to improve model performance.  

```{r, echo=FALSE}

lm_enhanced <- lm(price ~ lotSize + age + landValue + livingArea + bedrooms + bathrooms + rooms + heating + waterfront + newConstruction + centralAir + livingArea:bedrooms + log(livingArea), data=saratoga_train)
summary(lm_enhanced)

```

We proceeded by creating a model with the relevant variables remaining after the backwards selection process, then adding a relevant interaction term (living area:bedrooms) and one logarithmic transformation. We included an interaction of living area size and number of bedrooms because we suspected that the combination of living area size and the number of bedrooms together influence the price of a house in ways that considering each of these factors alone cannot capture. We also included a logarithmic transformation of the living area variable in order to capture how proportional changes in living area affect price, allowing for non-linearity in the relationship between these variables. 

```{r, echo=FALSE, error = FALSE, warnings = FALSE}
out = do(100)*{
  saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
  saratoga_train = training(saratoga_split)
  saratoga_test = testing(saratoga_split)
  lm2 = lm(price ~ (. - pctCollege - sewer - waterfront - landValue -
                      newConstruction), data=saratoga_train)
  
  lm_enhanced <- lm(price ~ lotSize + age + landValue + livingArea + bedrooms + bathrooms + rooms + heating + waterfront + newConstruction + centralAir + livingArea:bedrooms + log(livingArea), data=saratoga_train)
  rmse2 = rmse(lm2, saratoga_test)
  rmse_enhanced = rmse(lm_enhanced, saratoga_test)
  c(rmse2, rmse_enhanced)
}


average_rmse <- colMeans(out)

names(average_rmse) <- c("lm2", "lm_enhanced")

cat("Average Out-of-Sample RMSE Across 100 Train/Test Splits\n")
print(average_rmse)
```

As evidenced above, the average out-of-sample RMSE for our enhanced model is clearly lower than that of the previously considered medium model. We used the average RMSEs over 100 train/test splits in order to account for random variation in measuring out-of-sample performance. 

## Question 1: Part 2: Building the KNN Model

```{r, echo=FALSE}

# Define the features to include
features <- c("lotSize", "age", "landValue", "bedrooms", "bathrooms", 
              "rooms", "waterfrontNo", "newConstructionNo", 
              "centralAirNo", "HotWaterSteamHeatingYes")

# Prepare the dataset
# Generate Hot Water Steam Heating Variable
SaratogaHouses$HotWaterSteamHeatingYes <- ifelse(SaratogaHouses$heating == "hot water/steam", 1, 0)
SaratogaHouses$waterfrontNo <- ifelse(SaratogaHouses$waterfront == "No", 1, 0)
SaratogaHouses$newConstructionNo <- ifelse(SaratogaHouses$newConstruction == "No", 1, 0)
SaratogaHouses$centralAirNo <- ifelse(SaratogaHouses$centralAir == "No", 1, 0)

set.seed(123)
trainIndex <- createDataPartition(SaratogaHouses$price, p = .8, list = FALSE)
trainSet <- SaratogaHouses[trainIndex, c(features, "price")]
testSet <- SaratogaHouses[-trainIndex, c(features, "price")]

# Train
set.seed(123)
control <- trainControl(method = "repeatedcv", number = 10, repeats = 3, search = "random")
knnFit <- train(price ~ ., data = trainSet, method = "knn", preProcess = c("center", "scale"), trControl = control, tuneLength = 10)

# Evaluate
predictions <- predict(knnFit, newdata = testSet)
KNNperformance <- postResample(pred = predictions, obs = testSet$price)

# Output
print(knnFit)
print(KNNperformance)

```

### Model Comparison: Linear Model vs KNN 

```{r, echo=FALSE}

KNN_RMSE <- KNNperformance[1]

# Directly use `average_rmse` for lm2 and lm_enhanced
# Add the KNN RMSE to the existing `average_rmse` vector for consistency
average_rmse["KNN (k=14)"] <- KNN_RMSE

# Print the results with a title
cat("Average Out-of-Sample RMSE By Model")
print(average_rmse)
```

## Q1: Conclusion

Based on our analysis, the KNN model seems to do better at achieving a lower out-of-sample mean squared error, indicating a higher level of predictive accuracy relative to the linear model. This suggests that for the purpose of estimating house prices in Saratoga, NY, the KNN model might be the preferred choice. However, it's important to note that the difference in performance between the two models is relatively modest. The choice between these models should also consider factors such as interpretability and ease of implementation. While the KNN model offers marginally better accuracy, the enhanced linear model provides clearer insights into how specific features influence house prices, which could be valuable for policy formulation and decision-making by the local taxing authority. Considering the taxing authority's need to predict property values to determine tax policy, the slightly lower RMSE achieved by the KNN model suggests that the KNN model is performing marginally better at predicting property values in this context. 
 

# Question 2: German Credit Data


```{r, echo=FALSE, warning=FALSE,include=FALSE}
credit_data$Default <- as.factor(credit_data$Default)
credit_data$history <- factor(credit_data$history, levels = c("good", "poor", "terrible"))
credit_data$purpose <- as.factor(credit_data$purpose)
credit_data$foreign <- as.factor(credit_data$foreign)
```

```{r, echo=FALSE, warning=FALSE}
credit_history_summary <- credit_data %>%
  group_by(history) %>%
  summarise(Default_Probability = mean(as.numeric(Default) - 1)) %>%
  arrange(desc(Default_Probability))

ggplot(credit_history_summary, aes(x = history, y = Default_Probability, fill = history)) +
  geom_bar(stat = "identity") +
  labs(y = "Default Probability", x = "Class of Credit History", title = "Default Probability by Credit History") +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal() +
  theme(legend.position = "none")
```

```{r, echo=FALSE, warning=FALSE}
credit_train <- credit_data
logit_default <- glm(Default ~ duration + amount + installment + age + history + purpose + foreign, 
                     data = credit_train, 
                     family = binomial)

rounded_coefs <- round(coef(logit_default), 2)

print(rounded_coefs)
```

The  data and analysis from a German bank highlight a puzzling relationship between credit history and loan defaults. The bar plot indicates that worse credit history is associated with higher default rates. However, the logistic regression model suggests the opposite, with 'poor' and 'terrible' credit histories associated with lower default probabilities, which contradicts common financial intuition.

This contradiction is likely due to the bank's sampling method, which oversamples defaults. This method creates a bias, making the model less applicable to the general population of borrowers. For the model to be useful in screening new borrowers, the bank needs to use a sample that accurately reflects the true default rate in its overall portfolio. Correcting this sampling bias is essential for the bank to develop a reliable model for classifying borrowers by default risk.

# Question 3: Hotel Bookings

## Model Engineering

To estimate the performance of these 3 models, we will use K-fold cross validation with K = 10 to calculate the RMSE for each linear probability model. 


```{r, include = FALSE}
spec_reg = lm(children ~ market_segment+ adults + customer_type +is_repeated_guest, data=hotels_dev)
all_reg = lm(children ~ . -arrival_date, data=hotels_dev)
set.seed(2252024)
train.control = trainControl(method = "cv", number = 10)
spec_model = train(children ~ market_segment+ adults + customer_type + is_repeated_guest, data=hotels_dev, method = "lm", trControl = train.control)
print(spec_model)
spec_rmse = spec_model$results$RMSE
```

The 1st model has the following estimated RMSE:

```{r, include = TRUE}
spec_rmse
```

```{r, include = FALSE}
all_model = train(children ~ .-arrival_date, 
                  data=hotels_dev, 
                  method = "lm", 
                  trControl = train.control)
all_rmse = all_model$results$RMSE
```
The model with all the data provided (exlcuding arrival date) has the following estimated RMSE:
```{r, include = TRUE}
all_rmse
```
For the best performing model, we expanded off the all model and added several interaction terms and polynomials. We fitted adults to a 2nd degree polynomial, since increasing adults up to likely increases the chance of children coming but after the would decrease. We also fitted lead time to a 2nd degree polynomial, since reservations booked very early and very late would likely be business trips, as well as total number of special requests since families may make some requests for their children but a large amount indicates some special or business event. Finally, stay in weekend nights and stay in week nights were fitted to 2nd degree polynomials as well, as large amounts of both would indicate long term stays, which would likely not have children. I also added an interaction term for is_repeated_guest and previous_bookings_not_canceled, as that may indicate a repeat customer who makes sudden changes to their bookings, which would likely not have a child. Finally, I extracted several pieces of data from the timestamp of arrival. "Summer" marks whether the reservation was in the Summer, a time when children are more likely to be traveling. I also added week_end arrival, which tracked if the reservation started on Friday or Saturday. As people arriving on those days are likely traveling for vacation rather than business, it would be reasonable to assume children would be more likely to be on the reservation.
```{r, include = FALSE}
hotels_dev = hotels_dev %>%
  mutate(month = month(arrival_date),
         summer = ifelse(month >= 6 & month <= 8, 1, 0),
         week_day = wday(arrival_date),
         weekend_arrival = ifelse(week_day == 5|week_day == 6, 1, 0)
         )
```


```{r, include = FALSE}
engineered_model = train(children ~ market_segment+ poly(adults,2) + customer_type                                              +is_repeated_guest + is_repeated_guest*previous_cancellations + month + summer +adults*summer + poly(lead_time,2) +previous_cancellations + previous_bookings_not_canceled + previous_cancellations*previous_bookings_not_canceled + poly(stays_in_weekend_nights, 2) + meal + market_segment + distribution_channel + reserved_room_type + assigned_room_type + poly(booking_changes, 2) + deposit_type + days_in_waiting_list + poly(average_daily_rate,2) + required_car_parking_spaces +  poly(total_of_special_requests, 2)+  poly(stays_in_week_nights,2), 
                  data=hotels_dev, 
                  method = "lm", 
                  trControl = train.control)
engineered_rmse = engineered_model$results$RMSE

```
The RMSE for the engineered model is:
```{r, include = TRUE}
engineered_rmse
```
As can be seen, this model outperforms the other 2 in terms of RMSE.


## Data Validation

I am then moving on to validating the data with the hotels_val set. To do so, I graph the the model's Total Positivity Rate (TPR) and False Positivity Rate (FPR) as a function of t, the threshold for considering a predicted probability a yes.
```{r, include = FALSE}
hotels_val = read.csv("C://Users/jacob/Downloads/hotels_val.csv", header = TRUE)
hotels_val = hotels_val %>%
  mutate(month = month(arrival_date),
         summer = ifelse(month >= 6 & month <= 8, 1, 0),
         week_day = wday(arrival_date),
         weekend_arrival = ifelse(week_day == 5|week_day == 6, 1, 0)
         )
predict_val = predict(engineered_model, hotels_val)
roc_graph = roc(hotels_val$children, predict_val)
```

```{r, include = TRUE}
ggplot(data = coords(roc_graph), aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(color = "blue") +
  labs(title = "ROC Curve") +
  labs(x = "FPR(t)", y = "TPR(t)")  +
  theme_minimal()
```


## Fold test

After graphing the ROC curve, we move to testing model against 20 equal sized folds int he validation data set.
```{r, include = FALSE}
val_folds = createFolds(hotels_val$children, k=20, list = TRUE)
predict_booking = function(fold_indices, data) {
  data_fold = data[fold_indices,]
  predicted_probs = predict(engineered_model, newdata = data_fold)
  sum(predicted_probs)
}
predicted_bookings = foreach(i = seq_along(val_folds), .combine = "c") %dopar% {
  predict_booking(val_folds[[i]], hotels_val)
}
actual_bookings = sapply(val_folds, function(fold_indices) {
  sum(hotels_val$children[fold_indices])
})

comparison_data = data.frame(Fold = seq_along(val_folds), Actual_Bookings = actual_bookings, Predicted_Bookings = predicted_bookings)
```

The below figure summarizes this performance, with the red bars being the actual amount of bookings with children and the red the predicted amount of bookings. As can be seen, the model does moderately well at predicting the total number of bookings with children. While the model did not perfectly predict a fold, it was never substantially of (such as having a prediction off by 10 or more). Considering that each fold has about 250 bookings, that is not a substantial deviance.

```{R, include = TRUE}
graph_comparison = melt(comparison_data, id.vars = "Fold", variable.name = "Type", value.name = "Sum")

ggplot(graph_comparison, aes(x = factor(Fold), y = Sum, fill = Type)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8)) +
  labs(x = "Fold", y = "Bookings", title = "Comparison of Predicted vs. Actual Bookings") 
```

# Question 4: Poisonous Mushrooms

```{r, echo=FALSE,warning=FALSE}
mushrooms[] <- lapply(mushrooms, function(x) if(is.character(x)) factor(x) else x)


mushrooms <- mushrooms[, sapply(mushrooms, function(x) nlevels(x) > 1)]


dummies <- dummyVars(" ~ .", data = mushrooms)
mushrooms_transformed <- predict(dummies, newdata = mushrooms)


y <- mushrooms_transformed[, 1]
X <- mushrooms_transformed[, -1]


set.seed(123)  # For reproducibility
cv_model <- cv.glmnet(as.matrix(X), as.factor(y), alpha = 1, family = "binomial", nfolds = 10)


optimal_lambda <- cv_model$lambda.min


print(paste("Optimal lambda:", optimal_lambda))
```

```{r, echo=FALSE,warning=FALSE}

mushrooms <- mushrooms[sapply(mushrooms, function(x) length(unique(x))) > 1]
mushrooms[] <- lapply(mushrooms, factor)

set.seed(123) 
trainIndex <- createDataPartition(mushrooms$class, p = .8, 
                                  list = FALSE, times = 1)
mushrooms_train <- mushrooms[trainIndex, ]
mushrooms_test <- mushrooms[-trainIndex, ]

mushrooms_test[] <- lapply(names(mushrooms_test), function(name) {
  if (name %in% names(mushrooms_train)) {
    factor(mushrooms_test[[name]], levels = levels(mushrooms_train[[name]]))
  } else {
    mushrooms_test[[name]]
  }
})

train_matrix <- model.matrix(~ . - 1, data = mushrooms_train)
test_matrix <- model.matrix(~ . - 1, data = mushrooms_test)


if (ncol(train_matrix) != ncol(test_matrix)) {
  stop("The number of variables in the training and test sets do not match.")
}

train_class <- as.numeric(mushrooms_train$class) - 1
test_class <- as.numeric(mushrooms_test$class) - 1


set.seed(123) 
cv_model <- cv.glmnet(train_matrix, train_class, family = "binomial", alpha = 1)
```

```{r, echo=FALSE,warning=FALSE}
predictions <- predict(cv_model, newx = test_matrix, s = "lambda.min", type = "response")
roc_result <- roc(test_class, predictions)
plot(roc_result, main = "ROC Curve")
auc(roc_result)
```

```{r, echo=FALSE,warning=FALSE}
threshold <- 0.5 
predicted_class <- ifelse(predictions > threshold, 1, 0)
confusionMatrix <- table(Predicted = predicted_class, Actual = test_class)
print(confusionMatrix)
```
```{r,echo=FALSE,warning=FALSE}
set.seed(123)  
trainIndex <- createDataPartition(mushrooms$class, p = .7, list = FALSE, times = 1)
trainData <- mushrooms_transformed[trainIndex, ]
testData <- mushrooms_transformed[-trainIndex, ]


model <- glmnet(as.matrix(trainData[, -1]), as.factor(trainData[, 1]), alpha = 1, family = "binomial", lambda = optimal_lambda)


predictions <- predict(model, newx = as.matrix(testData[, -1]), type = "response")


predicted_class <- ifelse(predictions > 0.5, 1, 0)  # Adjust the threshold if necessary


conf_matrix <- table(predicted_class, testData[, 1])


TPR <- conf_matrix[2, 2] / (conf_matrix[2, 2] + conf_matrix[2, 1])  # True Positives / (True Positives + False Negatives)
FPR <- conf_matrix[1, 2] / (conf_matrix[1, 2] + conf_matrix[1, 1])  # False Positives / (False Positives + True Negatives)
```

```{r,echo=FALSE,warning=FALSE}

paste("True Positive Rate:", TPR)
paste("False Positive Rate:", FPR)
```

The document contains results from a Lasso regression model with 10-fold cross-validation, indicating an optimal lambda value of approximately 0.000676. The area under the ROC curve (AUC) is 1, suggesting perfect model performance. The confusion matrix shows no misclassifications, with 841 true negatives and 783 true positives, leading to a True Positive Rate (TPR) of 1 and a False Positive Rate (FPR) of 0. These results suggest that the model perfectly distinguishes between the classes without any errors, so a threshold of 1 would be recommended for declaring a mushroom poisonous.






